---
title: "Predicting how well a weight lifting exercise activity is completed using accelerometer data"
author: "Daniel Chaytor"
date: "November 27, 2016"
output: html_document
---

## Overview
This project explores data from accelerometers on the belt, forearm, arm, and dumbell of six participants that were asked to perform barbell lifts correctly and incorrectly in five different ways. The goal is to predict the manner in which (i.e., how well) they did the exercise. 
The six participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different ways: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A is the correct way, while the other classes represent incorrect ways of the performing the activity.

## Getting the data
Links to the data were provided. We first load the libraries to be used, then read in the data into training and test datasets:

```{r}
library(caret)
library(rpart)
library(rpart.plot)
library(rattle)
library(randomForest)
library(knitr)

trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))

dim(training)
dim(testing)
```

There are 160 variables in the datasets. 

## Data Cleaning

We start by cleaning the data, firstly to remove those variables with near zero variance, and then to remove all variables with missing values. We also remove some variables not relevant to the study, including those with timestamp data.

```{r}
nzv <- nearZeroVar(training, saveMetrics=TRUE)
training <- training[,nzv$nzv==FALSE]

nzv<- nearZeroVar(testing,saveMetrics=TRUE)
testing <- testing[,nzv$nzv==FALSE]

training <- training[,(colSums(is.na(training)) == 0)]
testing <- testing[,(colSums(is.na(testing)) == 0)]

training <- training[c(-1)] #remove first column
testing <- testing[c(-1)]   #remove first column

# Remove timestamp variables and the num_window variable
rmtimestampcols<-!grepl("*timestamp*",names(training))
training <- training[ ,rmtimestampcols]
rmOthers<-!grepl("*window*",names(training))
training <- training[ ,rmOthers]

rmtimestampcols<-!grepl("*timestamp*",names(testing))
testing <- testing[ ,rmtimestampcols]
rmOthers<-!grepl("*window*",names(testing))
testing <- testing[ ,rmOthers]

dim(training)
dim(testing)
```
The resulting datasets each have 54 variables.

## Subsetting Training data into training and test datasets 

Next, we subset the data into training and test sets. We will use 70% of the training dataset for training data and the remaining 30% as test data. 

```{r}
inTrain <- createDataPartition(training$classe, p=0.7, list=FALSE)
trainSet <- training[inTrain, ]
testSet <- training[-inTrain, ]
```

## Training model

We will start our predictions by using decision trees. 

### Decision Tree Model

```{r}
set.seed(12345)
#modFitA1 <- train(classe~.,method="rpart", data=trainSet)
#fancyRpartPlot(modFitA1$finalModel,cex=.5,under.cex=1,shadow.offset=0)
modFitA1 <- rpart(classe ~ ., data=trainSet, method="class")
fancyRpartPlot(modFitA1)

#Predict on the testing data and show confusion matrix
predictClasse <- predict(modFitA1,testSet, type="class")
confusionMatrix(predictClasse, testSet$classe)
```

After applying the model to the test set, the confusion matrix shows that the decision tree model gives an accuracy of about 74%, with the outcome B having the least accuracy.


### Random Forest Model

We will now build a random forest model and apply it to our data.

```{r}
modfitRF <- randomForest(classe ~ ., data=trainSet, importance=TRUE)
modfitRF
```

#### Cross Validation and Out of Sample Error
From the fitted random forest model, the OOB error rate is 0.55%. This basically means that the out of sample error is about 0.55%. According to the original random forests paper, it is not really necessary to perform cross validation when the random forest method is applied, since the error rate is estimated internally, and it is sufficiently unbiased.

#### Random forest prediction
We apply the random forest model to the test set and generate the confusion matrix.

```{r}
predictClasseRF <- predict(modfitRF, testSet, type = "class")
confusionMatrix(predictClasseRF, testSet$classe)
```

The random forest model has a much higher accuracy (99.5%) than the decision tree method. The outcome D has the lowest accuracy with this model.
 
We can also plot the importance of the different variables to the model.

```{r}
varImpPlot(modfitRF, sort = TRUE, type = 1, pch = 19, col = 1, cex = 0.6, main = "Importance of  Individual Variables")
```
The variables yaw_belt, roll_belt,pitch_belt and magnet_dumbell_z are shown to have the most impact on the result.

## Prediction of the 20 Test Cases

We can now apply this model with the 99.5% accuracy to the original test data downloaded, containing 20 observations.

```{r}
predictionTest <- predict(modfitRF, testing[, -length(names(testing))])
print(predictionTest)
```

##Conclusion
This project has used two different predictive models to determine the manner in which a particular exercice was performed.

Random Forests proved to give more accurate prediction results compared to decision trees. 

With decision trees B was the most difficult to predict and with Random forests, B was the most difficult to predict. Among the most important variables were yaw_belt, roll_belt and pitch_belt.
